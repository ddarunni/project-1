import os
import operator
from typing import TypedDict, Annotated, List, Any

from dotenv import load_dotenv
from langchain_core.messages import AnyMessage, HumanMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.agents import AgentExecutor, create_openai_functions_agent
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, END

# --- 1. ì„¤ì • (Configuration) ---
# ìŠ¤í¬ë¦½íŠ¸ì˜ ì£¼ìš” ì„¤ì •ì„ ìƒë‹¨ì— ìƒìˆ˜ë¡œ ì •ì˜í•˜ì—¬ ê´€ë¦¬í•©ë‹ˆë‹¤.
MODEL_NAME = "gpt-4o"
ROUTING_KEYWORDS = ["ì‚¬ì—…ì‹¤", "ê·¸ë£¹", "íŒë§¤ëŸ‰", "ë§¤ì¶œì•¡", "ì˜ì—…ì´ìµ", "ì„¸ì „ì´ìµ","ê³µê¸‰ì‚¬", "ê³ ê°ì‚¬", "êµ­ê°€"]

# í”„ë¡¬í”„íŠ¸ ë¡œë” import
try:
    from agent.prompt_loader import get_combined_prompt
    SYSTEM_PROMPT = get_combined_prompt()
except ImportError:
    SYSTEM_PROMPT = "ë„ˆëŠ” ì² ê°• ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì•¼. ì‚¬ìš©ì ì§ˆë¬¸ì— ë§ëŠ” ë„êµ¬ë¥¼ ì„ íƒí•´ì„œ ì •í™•í•œ ìˆ˜ì¹˜ë¥¼ í¬í•¨í•œ í•œêµ­ì–´ ë‹µë³€ì„ í•´ì¤˜."

# --- 2. ìƒíƒœ ì •ì˜ (State Definition) ---
# ê·¸ë˜í”„ ì „ì²´ì—ì„œ ì‚¬ìš©ë  ìƒíƒœ ê°ì²´ì˜ êµ¬ì¡°ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
class AgentState(TypedDict):
    input: str
    output: str
    chat_history: Annotated[List[AnyMessage], operator.add]
    df: Any
    # ë‹¤ì¤‘ ë°ì´í„°ì…‹ ê´€ë ¨
    datasets_info: dict  # ë‹¤ì¤‘ ë°ì´í„°ì…‹ ì •ë³´ {name: dataframe}
    dataset_count: int   # ì—…ë¡œë“œëœ ë°ì´í„°ì…‹ ìˆ˜
    is_multi_dataset: bool  # ë‹¤ì¤‘ íŒŒì¼ ì—¬ë¶€
    active_dataset_name: str  # í˜„ì¬ í™œì„± ë°ì´í„°ì…‹ ì´ë¦„
    # Context Aware Node ê´€ë ¨
    enhanced_input: str
    context_used: bool
    context_info: dict
    # Intent Classification Node ê´€ë ¨
    intent_info: dict
    processing_path: str
    # Query Planning Node ê´€ë ¨
    query_plan: dict

# --- 3. ì—ì´ì „íŠ¸ ë° ê·¸ë˜í”„ êµ¬ì„± ìš”ì†Œ (Agent & Graph Components) ---

def create_agent_executor(api_key: str, tools: list) -> AgentExecutor:
    """AgentExecutorë¥¼ ìƒì„±í•˜ê³  ë°˜í™˜í•©ë‹ˆë‹¤."""
    llm = ChatOpenAI(model=MODEL_NAME, temperature=0, openai_api_key=api_key)
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", SYSTEM_PROMPT),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
        MessagesPlaceholder("agent_scratchpad")
    ])
    
    agent = create_openai_functions_agent(llm=llm, tools=tools, prompt=prompt)
    
    return AgentExecutor(
        agent=agent,
        tools=tools,
        verbose=True,
        handle_parsing_errors=True # íŒŒì‹± ì—ëŸ¬ ë°œìƒ ì‹œ ì—ì´ì „íŠ¸ê°€ ê°•ê±´í•˜ê²Œ ëŒ€ì‘í•˜ë„ë¡ ì„¤ì •
    )

# --- ë…¸ë“œ í•¨ìˆ˜ë“¤ ---
def router_node(state: AgentState) -> dict:
    """ìƒíƒœë¥¼ ë³€ê²½í•˜ì§€ ì•ŠëŠ” ë¼ìš°íŒ… ì§„ì…ì  ë…¸ë“œì…ë‹ˆë‹¤."""
    print("--- ë¼ìš°í„° ì§„ì… ---")
    return {}

def context_aware_node(state: AgentState) -> dict:
    """ì´ì „ ëŒ€í™” ë§¥ë½ì„ í™œìš©í•˜ì—¬ í˜„ì¬ ì§ˆë¬¸ì„ í–¥ìƒì‹œí‚¤ëŠ” ë…¸ë“œì…ë‹ˆë‹¤."""
    print("--- Context Aware ë…¸ë“œ ì‹¤í–‰ ---")
    
    current_input = state["input"]
    chat_history = state.get("chat_history", [])
    
    # ì°¸ì¡°ì–´ íŒ¨í„´ ì •ì˜
    reference_patterns = ["ê·¸ê²ƒ", "ì´ì „", "ì•ì„œ", "ë”", "ë˜í•œ", "ê·¸ ê²°ê³¼", "ê·¸ëŸ°ë°", "ê·¸ë¦¬ê³ "]
    
    context_info = {
        "has_reference": False,
        "reference_type": None,
        "previous_entities": [],
        "enhancement_applied": False
    }
    
    enhanced_input = current_input
    context_used = False
    
    # ì°¸ì¡°ì–´ê°€ ìˆëŠ”ì§€ í™•ì¸
    has_references = any(ref in current_input for ref in reference_patterns)
    
    if has_references and chat_history:
        context_used = True
        context_info["has_reference"] = True
        
        # ìµœê·¼ ëŒ€í™”ì—ì„œ ì£¼ìš” ì—”í‹°í‹° ì¶”ì¶œ
        last_conversations = chat_history[-2:] if len(chat_history) >= 2 else chat_history
        entities = []
        
        for conv in last_conversations:
            if isinstance(conv, str):
                # ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ
                keywords = ["ì‚¬ì—…ì‹¤", "ê·¸ë£¹", "ë§¤ì¶œ", "ìˆ˜ëŸ‰", "ì˜ì—…ì´ìµ", "ì„¸ì „ì´ìµ"]
                for keyword in keywords:
                    if keyword in conv:
                        # í‚¤ì›Œë“œ ì•ì˜ ìˆ˜ì‹ì–´ê¹Œì§€ í¬í•¨í•´ì„œ ì¶”ì¶œ
                        words = conv.split()
                        for i, word in enumerate(words):
                            if keyword in word:
                                if i > 0 and len(words[i-1]) > 1:
                                    entity = f"{words[i-1]} {word}"
                                    entities.append(entity)
                                else:
                                    entities.append(word)
        
        context_info["previous_entities"] = entities
        
        # ì°¸ì¡°ì–´ í•´ê²°
        if "ê·¸ê²ƒ" in current_input and entities:
            main_entity = entities[-1] if entities else "ì´ì „ ê²°ê³¼"
            enhanced_input = current_input.replace("ê·¸ê²ƒ", main_entity)
            context_info["reference_type"] = "direct_reference"
            context_info["enhancement_applied"] = True
            
        elif any(word in current_input for word in ["ë”", "ë˜í•œ", "ì¶”ê°€ë¡œ"]) and entities:
            # ì•”ë¬µì  ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
            main_entity = entities[-1] if entities else ""
            if main_entity and main_entity not in current_input:
                enhanced_input = f"{main_entity}ì˜ {current_input}"
                context_info["reference_type"] = "implicit_context"
                context_info["enhancement_applied"] = True
    
    print(f"ì›ë³¸ ì§ˆë¬¸: {current_input}")
    print(f"í–¥ìƒëœ ì§ˆë¬¸: {enhanced_input}")
    print(f"ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©: {context_used}")
    
    return {
        "enhanced_input": enhanced_input,
        "context_used": context_used,
        "context_info": context_info
    }

def intent_classification_node(state: AgentState) -> dict:
    """ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ ë¶„ë¥˜í•˜ê³  ì²˜ë¦¬ ê²½ë¡œë¥¼ ê²°ì •í•˜ëŠ” ë…¸ë“œì…ë‹ˆë‹¤."""
    print("--- Intent Classification ë…¸ë“œ ì‹¤í–‰ ---")
    
    # Context Aware Nodeì—ì„œ í–¥ìƒëœ ì§ˆë¬¸ì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ì›ë³¸ ì‚¬ìš©
    input_text = state.get("enhanced_input", state["input"]).lower()
    
    # ì˜ë„ ë¶„ë¥˜ ì¹´í…Œê³ ë¦¬ ì •ì˜
    intent_patterns = {
        "aggregation": ["í•©ê³„", "ì´", "ì „ì²´", "ëª¨ë“ ", "ì „ë¶€", "sum", "total"],
        "comparison": ["ë¹„êµ", "vs", "ëŒ€ë¹„", "ì°¨ì´", "ë¹„í•´", "ë³´ë‹¤"],
        "ranking": ["ìƒìœ„", "í•˜ìœ„", "ìˆœìœ„", "ê°€ì¥", "ìµœê³ ", "ìµœì €", "top", "ë†’ì€", "ë‚®ì€"],
        "trend": ["ì¶”ì„¸", "ë³€í™”", "ì¦ê°€", "ê°ì†Œ", "ì‹œê°„ë³„", "ê¸°ê°„ë³„", "ë…„ë„ë³„", "ì›”ë³„"],
        "filtering": ["ì¡°ê±´", "í•„í„°", "~ì¸", "~ë§Œ", "~ì¤‘ì—ì„œ", "~ì—ì„œ"],
        "statistical": ["í‰ê· ", "ìµœëŒ€", "ìµœì†Œ", "í‘œì¤€í¸ì°¨", "ë¶„ì‚°", "ì¤‘ì•™ê°’"]
    }
    
    # ë³µì¡ë„ ì§€í‘œ ì •ì˜
    complexity_indicators = {
        "high": ["ê·¸ë£¹ë³„ë¡œ", "ì„¸ë¶„í™”", "ìƒì„¸íˆ", "ê°ê°", "ë¹„êµë¶„ì„", "ë¶„ì„í•´", "ìì„¸íˆ"],
        "medium": ["ê¸°ê°„ë³„", "ì›”ë³„", "ë¶„ê¸°ë³„", "ì—°ë„ë³„", "ë‚˜ëˆ„ì–´", "êµ¬ë¶„"],
        "low": ["ì´", "ì „ì²´", "í•©ê³„", "ê°œìˆ˜", "ì–¼ë§ˆ", "ëª‡"]
    }
    
    # ì˜ë„ ë¶„ë¥˜
    detected_intent = "general"
    confidence = 0.0
    
    for intent, patterns in intent_patterns.items():
        matches = sum(1 for pattern in patterns if pattern in input_text)
        if matches > 0:
            current_confidence = matches / len(patterns)
            if current_confidence > confidence:
                detected_intent = intent
                confidence = current_confidence
    
    # ë³µì¡ë„ ë¶„ë¥˜
    complexity = "medium"  # ê¸°ë³¸ê°’
    complexity_score = 0
    
    for level, indicators in complexity_indicators.items():
        matches = sum(1 for indicator in indicators if indicator in input_text)
        if matches > complexity_score:
            complexity = level
            complexity_score = matches
    
    # í•„ìš”í•œ ë„êµ¬ ì˜ˆì¸¡
    tool_predictions = {
        "aggregation": ["calculate_sum", "calculate_total", "group_statistics"],
        "comparison": ["compare_groups", "calculate_percentage", "filter_data"],
        "ranking": ["sort_by_column", "get_top_n", "calculate_rank"],
        "trend": ["time_series_analysis", "calculate_growth"],
        "filtering": ["filter_data", "search_data"],
        "statistical": ["calculate_statistics", "describe_data"]
    }
    
    predicted_tools = tool_predictions.get(detected_intent, ["general_analysis"])
    
    # ì²˜ë¦¬ ê²½ë¡œ ê²°ì •
    processing_path = "agent_node"  # ê¸°ë³¸ê°’
    
    if detected_intent == "aggregation" and complexity == "low":
        processing_path = "quick_answer_node"  # í–¥í›„ êµ¬í˜„ ì˜ˆì •
    elif not any(keyword in input_text for keyword in ROUTING_KEYWORDS):
        processing_path = "fallback_node"
    
    intent_info = {
        "intent": detected_intent,
        "confidence": confidence,
        "complexity": complexity,
        "predicted_tools": predicted_tools,
        "processing_path": processing_path,
        "analysis": {
            "input_length": len(input_text),
            "keyword_matches": sum(1 for keyword in ROUTING_KEYWORDS if keyword in input_text)
        }
    }
    
    print(f"ê°ì§€ëœ ì˜ë„: {detected_intent} (ì‹ ë¢°ë„: {confidence:.2f})")
    print(f"ë³µì¡ë„: {complexity}")
    print(f"ì²˜ë¦¬ ê²½ë¡œ: {processing_path}")
    
    return {
        "intent_info": intent_info,
        "processing_path": processing_path
    }

def query_planning_node(state: AgentState) -> dict:
    """ë³µì¡í•œ ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ìµœì ì˜ ì‹¤í–‰ ê³„íšì„ ìˆ˜ë¦½í•©ë‹ˆë‹¤."""
    print("--- Query Planning ë…¸ë“œ ì‹¤í–‰ ---")
    
    # Context Aware Nodeì—ì„œ í–¥ìƒëœ ì§ˆë¬¸ì´ ìˆìœ¼ë©´ ì‚¬ìš©
    input_text = state.get("enhanced_input", state["input"]).lower()
    intent_info = state.get("intent_info", {})
    
    # ì»¬ëŸ¼-í‚¤ì›Œë“œ ë§¤í•‘ (tools.pyì™€ ë™ì¼)
    column_keywords = {
        "Division": ["ì‚¬ì—…ì‹¤", "ì‚¬ì—…ë¶€", "ë¶€ë¬¸", "ìŠ¤í…Œì¸ë¦¬ìŠ¤", "ì „ê¸°ê°•íŒ", "ì—´ì—°", "ëƒ‰ì—°"],
        "Country": ["êµ­ê°€", "ë‚˜ë¼", "í•œêµ­", "ì¤‘êµ­", "ì¼ë³¸", "ë¯¸êµ­", "êµ­ë‚´", "í•´ì™¸"],
        "Period/Year": ["ë…„", "ë…„ë„", "ë¶„ê¸°", "ìƒë°˜ê¸°", "í•˜ë°˜ê¸°", "2023", "2024", "2022"],
        "Supplier": ["ê³µê¸‰ì‚¬", "ê³µê¸‰ì—…ì²´", "posco", "í¬ìŠ¤ì½”"],
        "FundsCenter": ["ê·¸ë£¹", "ì„¼í„°", "funds", "í€ë“œ"]
    }
    
    metric_keywords = {
        "ë§¤ì¶œìˆ˜ëŸ‰(M/T)": ["ë§¤ì¶œìˆ˜ëŸ‰", "íŒë§¤ëŸ‰", "ìˆ˜ëŸ‰", "í†¤"],
        "1.ë§¤ì¶œì•¡": ["ë§¤ì¶œì•¡", "ë§¤ì¶œ", "sales"],
        "5.ì˜ì—…ì´ìµ": ["ì˜ì—…ì´ìµ", "ì´ìµ", "profit"],
        "8.ì„¸ì „ì´ìµ": ["ì„¸ì „ì´ìµ", "ì„¸ì „"]
    }
    
    # í•„ìš”í•œ ì»¬ëŸ¼ë“¤ ê°ì§€
    required_columns = []
    detected_metrics = []
    
    for column, keywords in column_keywords.items():
        if any(keyword in input_text for keyword in keywords):
            required_columns.append(column)
    
    for metric, keywords in metric_keywords.items():
        if any(keyword in input_text for keyword in keywords):
            detected_metrics.append(metric)
    
    # ê¸°ë³¸ ë©”íŠ¸ë¦­ ì„¤ì •
    if not detected_metrics:
        detected_metrics = ["ë§¤ì¶œìˆ˜ëŸ‰(M/T)"]
    
    # ì§ˆë¬¸ ë³µì¡ë„ ë¶„ì„
    complexity_score = 0
    complexity_indicators = {
        "multiple_filters": len(required_columns) >= 2,
        "comparison": any(word in input_text for word in ["ë¹„êµ", "vs", "ëŒ€ë¹„", "ì°¨ì´"]),
        "ranking": any(word in input_text for word in ["ìƒìœ„", "í•˜ìœ„", "ìˆœìœ„", "ê°€ì¥"]),
        "temporal": any(word in input_text for word in ["ë…„ë„ë³„", "ê¸°ê°„ë³„", "ì¶”ì„¸", "ë³€í™”"]),
        "aggregation": any(word in input_text for word in ["í•©ê³„", "ì´", "ì „ì²´", "ëª¨ë“ "])
    }
    
    complexity_score = sum(complexity_indicators.values())
    
    if complexity_score >= 3:
        query_complexity = "high"
    elif complexity_score >= 1:
        query_complexity = "medium"
    else:
        query_complexity = "low"
    
    # ì‹¤í–‰ ê³„íš ìˆ˜ë¦½
    execution_plan = {
        "strategy": "unknown",
        "recommended_tools": [],
        "parameters": {},
        "fallback_plan": "use_existing_tools"
    }
    
    # ë¹„êµ ë¶„ì„ ê³„íš
    if complexity_indicators["comparison"]:
        execution_plan["strategy"] = "comparative_analysis"
        execution_plan["recommended_tools"] = ["comparative_analysis_tool"]
        
        # ë¹„êµ ëŒ€ìƒ ì¶”ì¶œ ì‹œë„
        comparison_entities = extract_comparison_entities(input_text)
        execution_plan["parameters"] = {
            "comparison_detected": True,
            "entities": comparison_entities,
            "primary_metric": detected_metrics[0]
        }
    
    # ë‹¤ì¤‘ í•„í„° ê³„íš
    elif len(required_columns) >= 2:
        execution_plan["strategy"] = "multi_column_query"
        execution_plan["recommended_tools"] = ["advanced_multi_column_query"]
        execution_plan["parameters"] = {
            "filter_columns": required_columns,
            "metrics": detected_metrics,
            "complexity": query_complexity
        }
    
    # ë‹¨ì¼ ì¡°ê±´ ê³„íš
    elif len(required_columns) == 1:
        execution_plan["strategy"] = "single_column_query" 
        execution_plan["recommended_tools"] = ["existing_single_tools"]
        execution_plan["parameters"] = {
            "column": required_columns[0],
            "metric": detected_metrics[0]
        }
    
    # ì¼ë°˜ ê³„íš
    else:
        execution_plan["strategy"] = "general_analysis"
        execution_plan["recommended_tools"] = ["get_overall_summary"]
    
    query_plan = {
        "required_columns": required_columns,
        "detected_metrics": detected_metrics, 
        "complexity": query_complexity,
        "complexity_indicators": complexity_indicators,
        "execution_plan": execution_plan,
        "confidence": min(0.9, 0.3 + 0.2 * len(required_columns))
    }
    
    print(f"ë¶„ì„ëœ ì»¬ëŸ¼: {required_columns}")
    print(f"ê°ì§€ëœ ë©”íŠ¸ë¦­: {detected_metrics}")
    print(f"ë³µì¡ë„: {query_complexity}")
    print(f"ì¶”ì²œ ì „ëµ: {execution_plan['strategy']}")
    
    return {
        "query_plan": query_plan
    }

def multi_context_aware_node(state: AgentState) -> dict:
    """ë‹¤ì¤‘ ë°ì´í„°ì…‹ ì „ìš© ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ë…¸ë“œì…ë‹ˆë‹¤."""
    print("--- Multi Dataset Context Aware ë…¸ë“œ ì‹¤í–‰ ---")
    
    current_input = state["input"]
    chat_history = state.get("chat_history", [])
    datasets_info = state.get("datasets_info", {})
    
    # ë‹¤ì¤‘ íŒŒì¼ ê´€ë ¨ ì°¸ì¡°ì–´ íŒ¨í„´
    multi_ref_patterns = ["ë‘ íŒŒì¼", "ê° íŒŒì¼", "íŒŒì¼ë“¤", "ë¹„êµ", "ì°¨ì´", "ëª¨ë“ ", "ì „ì²´"]
    
    context_info = {
        "has_reference": False,
        "multi_dataset_context": True,
        "available_datasets": list(datasets_info.keys()),
        "enhancement_applied": False
    }
    
    enhanced_input = current_input
    context_used = False
    
    # ë‹¤ì¤‘ íŒŒì¼ ë§¥ë½ì—ì„œ ì°¸ì¡°ì–´ í•´ê²°
    has_multi_references = any(ref in current_input for ref in multi_ref_patterns)
    
    if has_multi_references:
        context_used = True
        context_info["has_reference"] = True
        
        # ë°ì´í„°ì…‹ ì •ë³´ë¥¼ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€
        if len(datasets_info) >= 2:
            dataset_hint = f"\n\nğŸ’¡ ì—…ë¡œë“œëœ ë°ì´í„°ì…‹: {', '.join(list(datasets_info.keys())[:3])}"
            if len(datasets_info) > 3:
                dataset_hint += f" ì™¸ {len(datasets_info)-3}ê°œ"
            enhanced_input = current_input + dataset_hint
            context_info["enhancement_applied"] = True
    
    print(f"ë‹¤ì¤‘ íŒŒì¼ ì›ë³¸ ì§ˆë¬¸: {current_input}")
    print(f"ë‹¤ì¤‘ íŒŒì¼ í–¥ìƒëœ ì§ˆë¬¸: {enhanced_input}")
    print(f"ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©: {context_used}")
    
    return {
        "enhanced_input": enhanced_input,
        "context_used": context_used,
        "context_info": context_info
    }

def multi_intent_classification_node(state: AgentState) -> dict:
    """ë‹¤ì¤‘ ë°ì´í„°ì…‹ ì „ìš© ì˜ë„ ë¶„ë¥˜ ë…¸ë“œì…ë‹ˆë‹¤."""
    print("--- Multi Dataset Intent Classification ë…¸ë“œ ì‹¤í–‰ ---")
    
    input_text = state.get("enhanced_input", state["input"]).lower()
    datasets_info = state.get("datasets_info", {})
    
    # ë‹¤ì¤‘ ë°ì´í„°ì…‹ íŠ¹í™” ì˜ë„ íŒ¨í„´
    multi_intent_patterns = {
        "comparison": ["ë¹„êµ", "compare", "vs", "ëŒ€ë¹„", "ì°¨ì´", "ë¹„í•´", "ë³´ë‹¤", "ê°„ì˜"],
        "aggregation": ["í•©ê³„", "ì´", "ì „ì²´", "ëª¨ë“ ", "í†µí•©", "ì¢…í•©"],
        "ranking": ["ìƒìœ„", "í•˜ìœ„", "ìˆœìœ„", "ê°€ì¥", "ìµœê³ ", "ìµœì €", "top", "ë†’ì€", "ë‚®ì€"],
        "cross_analysis": ["ê°ê°", "íŒŒì¼ë³„", "ë°ì´í„°ì…‹ë³„", "ì„œë¡œ", "ìƒí˜¸"],
        "summary": ["ìš”ì•½", "ê°œìš”", "ì „ë°˜ì ", "ì¢…í•©ì "]
    }
    
    detected_intent = "multi_general"
    confidence = 0.0
    
    for intent, patterns in multi_intent_patterns.items():
        matches = sum(1 for pattern in patterns if pattern in input_text)
        if matches > 0:
            current_confidence = matches / len(patterns)
            if current_confidence > confidence:
                detected_intent = intent
                confidence = current_confidence
    
    # ë‹¤ì¤‘ ë°ì´í„°ì…‹ ë³µì¡ë„ ë¶„ì„
    complexity_indicators = {
        "high": len(datasets_info) >= 3,  # 3ê°œ ì´ìƒ íŒŒì¼
        "comparison_complex": any(word in input_text for word in ["ì„¸ë¶€", "ìƒì„¸", "ë¶„ì„"]),
        "cross_reference": any(word in input_text for word in ["ê°ê°", "ì„œë¡œ", "ê°„ì˜"])
    }
    
    complexity = "high" if any(complexity_indicators.values()) else "medium"
    
    # ë‹¤ì¤‘ ë°ì´í„°ì…‹ ì „ìš© ë„êµ¬ ì¶”ì²œ
    tool_predictions = {
        "comparison": ["compare_datasets_summary", "compare_datasets_metrics", "compare_datasets_by_division"],
        "aggregation": ["integrated_dataset_analysis"],
        "cross_analysis": ["cross_dataset_comparison", "compare_datasets_by_division"],
        "summary": ["compare_datasets_summary"]
    }
    
    predicted_tools = tool_predictions.get(detected_intent, ["compare_datasets_summary"])
    
    intent_info = {
        "intent": detected_intent,
        "confidence": confidence,
        "complexity": complexity,
        "predicted_tools": predicted_tools,
        "processing_path": "multi_dataset_agent",
        "dataset_count": len(datasets_info),
        "multi_dataset_specific": True
    }
    
    print(f"ë‹¤ì¤‘ ë°ì´í„°ì…‹ ì˜ë„: {detected_intent} (ì‹ ë¢°ë„: {confidence:.2f})")
    print(f"ë³µì¡ë„: {complexity}, ë°ì´í„°ì…‹ ìˆ˜: {len(datasets_info)}")
    
    return {
        "intent_info": intent_info,
        "processing_path": "multi_dataset_agent"
    }

def multi_query_planning_node(state: AgentState) -> dict:
    """ë‹¤ì¤‘ ë°ì´í„°ì…‹ ì „ìš© ì¿¼ë¦¬ ê³„íš ë…¸ë“œì…ë‹ˆë‹¤."""
    print("--- Multi Dataset Query Planning ë…¸ë“œ ì‹¤í–‰ ---")
    
    input_text = state.get("enhanced_input", state["input"]).lower()
    intent_info = state.get("intent_info", {})
    datasets_info = state.get("datasets_info", {})
    
    # ë‹¤ì¤‘ ë°ì´í„°ì…‹ ì‹¤í–‰ ê³„íš
    execution_plan = {
        "strategy": "multi_dataset_analysis",
        "recommended_tools": [],
        "parameters": {
            "dataset_count": len(datasets_info),
            "dataset_names": list(datasets_info.keys())
        }
    }
    
    detected_intent = intent_info.get("intent", "multi_general")
    
    # ì˜ë„ë³„ ì‹¤í–‰ ê³„íš
    if detected_intent == "comparison":
        execution_plan["strategy"] = "dataset_comparison"
        execution_plan["recommended_tools"] = ["compare_datasets_metrics", "compare_datasets_by_division"]
        
    elif detected_intent == "aggregation":
        execution_plan["strategy"] = "dataset_integration"
        execution_plan["recommended_tools"] = ["integrated_dataset_analysis"]
        
    elif detected_intent == "cross_analysis":
        execution_plan["strategy"] = "cross_dataset_analysis"  
        execution_plan["recommended_tools"] = ["cross_dataset_comparison"]
        
    else:
        execution_plan["strategy"] = "multi_dataset_summary"
        execution_plan["recommended_tools"] = ["compare_datasets_summary"]
    
    query_plan = {
        "strategy": execution_plan["strategy"],
        "execution_plan": execution_plan,
        "confidence": min(0.9, 0.5 + 0.1 * len(datasets_info)),
        "multi_dataset_optimized": True,
        "dataset_info": {
            "count": len(datasets_info),
            "names": list(datasets_info.keys())[:3]  # ì²˜ìŒ 3ê°œë§Œ
        }
    }
    
    print(f"ë‹¤ì¤‘ ë°ì´í„°ì…‹ ì „ëµ: {execution_plan['strategy']}")
    print(f"ì¶”ì²œ ë„êµ¬: {execution_plan['recommended_tools']}")
    
    return {
        "query_plan": query_plan
    }

def dataset_routing_node(state: AgentState) -> str:
    """ë‹¨ì¼/ë‹¤ì¤‘ íŒŒì¼ì— ë”°ë¼ ì²˜ë¦¬ ê²½ë¡œë¥¼ ë¶„ê¸°í•©ë‹ˆë‹¤."""
    print("--- Dataset Routing ë…¸ë“œ ì‹¤í–‰ ---")
    
    dataset_count = state.get("dataset_count", 1)
    is_multi_dataset = state.get("is_multi_dataset", False)
    input_text = state.get("enhanced_input", state["input"]).lower()
    
    # ë‹¤ì¤‘ íŒŒì¼ ê´€ë ¨ í‚¤ì›Œë“œ ê°ì§€
    multi_dataset_keywords = [
        "ë¹„êµ", "compare", "vs", "ëŒ€ë¹„", "ì°¨ì´", "íŒŒì¼", "ë°ì´í„°ì…‹", 
        "ê°ê°", "ì„œë¡œ", "ê°„ì˜", "ì „ì²´", "í†µí•©", "ëª¨ë“ ", "ë‘", "ëª¨ë‘"
    ]
    
    has_comparison_intent = any(keyword in input_text for keyword in multi_dataset_keywords)
    
    print(f"ë°ì´í„°ì…‹ ìˆ˜: {dataset_count}")
    print(f"ë‹¤ì¤‘ íŒŒì¼ ì—¬ë¶€: {is_multi_dataset}")
    print(f"ë¹„êµ ì˜ë„ ê°ì§€: {has_comparison_intent}")
    
    # ë¶„ê¸° ë¡œì§
    if dataset_count > 1 and (is_multi_dataset or has_comparison_intent):
        print(">> ê²½ë¡œ: ë‹¤ì¤‘ ë°ì´í„°ì…‹ ê²½ë¡œ")
        return "multi_dataset_path"
    else:
        print(">> ê²½ë¡œ: ë‹¨ì¼ ë°ì´í„°ì…‹ ê²½ë¡œ")
        return "single_dataset_path"

def single_dataset_agent(state: AgentState, agent_executor: AgentExecutor) -> dict:
    """ë‹¨ì¼ ë°ì´í„°ì…‹ ì „ìš© ì²˜ë¦¬ ë…¸ë“œì…ë‹ˆë‹¤."""
    print("--- ë‹¨ì¼ ë°ì´í„°ì…‹ ì—ì´ì „íŠ¸ ì‹¤í–‰ ---")
    
    # ê¸°ì¡´ agent_nodeì™€ ë™ì¼í•œ ë¡œì§
    import agent.tools as tools_module
    if hasattr(tools_module, 'set_dataframe'):
        tools_module.set_dataframe(state["df"])
    
    input_to_use = state.get("enhanced_input", state["input"])
    print(f"ì‚¬ìš©í•  ì…ë ¥: {input_to_use}")
    
    result = agent_executor.invoke({
        "input": input_to_use,
        "chat_history": state["chat_history"]
    })
    
    # ë‹¨ì¼ íŒŒì¼ ì¶œì²˜ ì •ë³´
    df = state.get("df")
    dataset_name = state.get("active_dataset_name", "ì—…ë¡œë“œëœ íŒŒì¼")
    sheet_info = f"ğŸ“Š **ë°ì´í„° ì¶œì²˜:** {dataset_name}"
    
    if df is not None:
        sheet_info += f" (ì´ {len(df):,}í–‰, {len(df.columns)}ê°œ ì»¬ëŸ¼)"
    
    return {
        "output": result["output"],
        "intermediate_steps": result.get("intermediate_steps", []),
        "source_info": sheet_info
    }

def multi_dataset_agent(state: AgentState, agent_executor: AgentExecutor) -> dict:
    """ë‹¤ì¤‘ ë°ì´í„°ì…‹ ì „ìš© ì²˜ë¦¬ ë…¸ë“œì…ë‹ˆë‹¤."""
    print("--- ë‹¤ì¤‘ ë°ì´í„°ì…‹ ì—ì´ì „íŠ¸ ì‹¤í–‰ ---")
    
    datasets_info = state.get("datasets_info", {})
    input_text = state.get("enhanced_input", state["input"]).lower()
    
    # ë‹¤ì¤‘ ë°ì´í„°ì…‹ ë„êµ¬ ì„¤ì •
    import agent.tools as tools_module
    if hasattr(tools_module, 'set_datasets'):
        tools_module.set_datasets(datasets_info)
        print(f"ë‹¤ì¤‘ ë°ì´í„°ì…‹ ì„¤ì • ì™„ë£Œ: {list(datasets_info.keys())}")
    
    # í˜„ì¬ í™œì„± ë°ì´í„°ì…‹ë„ ë‹¨ì¼ ë„êµ¬ë“¤ì„ ìœ„í•´ ì„¤ì •
    if hasattr(tools_module, 'set_dataframe') and state.get("df") is not None:
        tools_module.set_dataframe(state["df"])
    
    input_to_use = state.get("enhanced_input", state["input"])
    
    # ë¹„êµ ë¶„ì„ ì˜ë„ê°€ ëª…í™•í•œ ê²½ìš° íŒíŠ¸ ì¶”ê°€
    comparison_keywords = ["ë¹„êµ", "compare", "vs", "ëŒ€ë¹„", "ì°¨ì´"]
    has_comparison = any(keyword in input_text for keyword in comparison_keywords)
    
    if has_comparison and len(datasets_info) >= 2:
        comparison_hint = f"\n\nğŸ’¡ ì°¸ê³ : í˜„ì¬ {len(datasets_info)}ê°œ ë°ì´í„°ì…‹ ì—…ë¡œë“œë¨ - "
        comparison_hint += "compare_datasets_summary(), compare_datasets_metrics(), compare_datasets_by_division() ë“±ì˜ ë„êµ¬ ì‚¬ìš© ê¶Œì¥"
        input_to_use += comparison_hint
    
    print(f"ë‹¤ì¤‘ ë°ì´í„°ì…‹ ì§ˆë¬¸: {input_to_use}")
    
    result = agent_executor.invoke({
        "input": input_to_use,
        "chat_history": state["chat_history"]
    })
    
    # ë‹¤ì¤‘ íŒŒì¼ ì¶œì²˜ ì •ë³´
    source_info = f"ğŸ“Š **ë°ì´í„° ì¶œì²˜:** {len(datasets_info)}ê°œ ë°ì´í„°ì…‹\n"
    for name, df in datasets_info.items():
        source_info += f"  â€¢ {name}: {len(df):,}í–‰ Ã— {len(df.columns)}ê°œ ì»¬ëŸ¼\n"
    
    return {
        "output": result["output"],
        "intermediate_steps": result.get("intermediate_steps", []),
        "source_info": source_info.strip()
    }

def dataset_comparison_node(state: AgentState) -> dict:
    """ë°ì´í„°ì…‹ ê°„ ë¹„êµ ì „ìš© ë…¸ë“œì…ë‹ˆë‹¤."""
    print("--- ë°ì´í„°ì…‹ ë¹„êµ ë…¸ë“œ ì‹¤í–‰ ---")
    
    datasets_info = state.get("datasets_info", {})
    
    if len(datasets_info) < 2:
        return {
            "output": "âŒ ë°ì´í„°ì…‹ ë¹„êµë¥¼ ìœ„í•´ì„œëŠ” ìµœì†Œ 2ê°œì˜ íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤.",
            "source_info": "ë¹„êµ ë¶„ì„ ì‹¤íŒ¨"
        }
    
    # ê¸°ë³¸ ë¹„êµ ë¶„ì„ ìˆ˜í–‰
    import agent.tools as tools_module
    if hasattr(tools_module, 'set_datasets'):
        tools_module.set_datasets(datasets_info)
    
    try:
        # compare_datasets_summary ë„êµ¬ ì§ì ‘ í˜¸ì¶œ
        if hasattr(tools_module, 'compare_datasets_summary'):
            summary_result = tools_module.compare_datasets_summary()
        else:
            summary_result = "ë¹„êµ ë„êµ¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # ì¶”ê°€ ë©”íŠ¸ë¦­ ë¹„êµ
        if hasattr(tools_module, 'compare_datasets_metrics'):
            metrics_result = tools_module.compare_datasets_metrics("ë§¤ì¶œìˆ˜ëŸ‰(M/T)")
            full_result = f"{summary_result}\n\n{metrics_result}"
        else:
            full_result = summary_result
        
        return {
            "output": full_result,
            "source_info": f"ğŸ“Š {len(datasets_info)}ê°œ ë°ì´í„°ì…‹ ìë™ ë¹„êµ ë¶„ì„ ì™„ë£Œ"
        }
        
    except Exception as e:
        return {
            "output": f"âŒ ë°ì´í„°ì…‹ ë¹„êµ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
            "source_info": "ë¹„êµ ë¶„ì„ ì˜¤ë¥˜"
        }

def extract_comparison_entities(text: str) -> list:
    """ë¹„êµ ëŒ€ìƒ ì—”í‹°í‹°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    entities = []
    
    # ê°„ë‹¨í•œ íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ë¹„êµ ëŒ€ìƒ ì¶”ì¶œ
    comparison_patterns = [
        r'(\w+)\s*vs\s*(\w+)',
        r'(\w+)\s*ê³¼\s*(\w+)\s*ë¹„êµ',
        r'(\w+)\s*ì™€\s*(\w+)\s*ë¹„êµ',
        r'(\w+)\s*ëŒ€ë¹„\s*(\w+)'
    ]
    
    import re
    for pattern in comparison_patterns:
        matches = re.findall(pattern, text)
        for match in matches:
            entities.extend(match)
    
    return entities[:4]  # ìµœëŒ€ 4ê°œê¹Œì§€ë§Œ

def enhanced_route_logic(state: AgentState) -> str:
    """Intent Classification ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì²˜ë¦¬ ê²½ë¡œë¥¼ ê²°ì •í•˜ëŠ” í–¥ìƒëœ ë¼ìš°íŒ… ë¡œì§ì…ë‹ˆë‹¤."""
    intent_info = state.get("intent_info", {})
    processing_path = intent_info.get("processing_path", "agent_node")
    
    print(f">> Intent ê¸°ë°˜ ê²½ë¡œ: {processing_path}")
    
    # í˜„ì¬ëŠ” quick_answer_nodeê°€ ì—†ìœ¼ë¯€ë¡œ agent_nodeë¡œ ë¼ìš°íŒ…
    if processing_path == "quick_answer_node":
        return "agent_node"
    
    return processing_path

def route_logic(state: AgentState) -> str:
    """ê¸°ì¡´ ë¼ìš°íŒ… ë¡œì§ (í˜¸í™˜ì„± ìœ ì§€ìš©)"""
    user_input = state["input"]
    print(f"ì…ë ¥ ë¶„ì„: '{user_input}'")
    if any(keyword in user_input for keyword in ROUTING_KEYWORDS):
        print(">> ê²½ë¡œ: ì—ì´ì „íŠ¸ ë…¸ë“œ")
        return "agent_node"
    else:
        print(">> ê²½ë¡œ: í´ë°± ë…¸ë“œ")
        return "fallback_node"

def agent_node(state: AgentState, agent_executor: AgentExecutor) -> dict:
    """ë°ì´í„° ë¶„ì„ì„ ìˆ˜í–‰í•˜ëŠ” ì—ì´ì „íŠ¸ ë…¸ë“œì…ë‹ˆë‹¤."""
    print("--- ì—ì´ì „íŠ¸ ë…¸ë“œ ì‹¤í–‰ ---")
    
    # dfë¥¼ ì „ì—­ ë³€ìˆ˜ë¡œ ì„¤ì •í•˜ì—¬ toolsì—ì„œ ì ‘ê·¼ ê°€ëŠ¥í•˜ë„ë¡ í•¨
    import agent.tools as tools_module
    if hasattr(tools_module, 'set_dataframe'):
        tools_module.set_dataframe(state["df"])
    
    # Context Aware Nodeì—ì„œ í–¥ìƒëœ ì§ˆë¬¸ì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ì›ë³¸ ì‚¬ìš©
    input_to_use = state.get("enhanced_input", state["input"])
    
    print(f"ì‚¬ìš©í•  ì…ë ¥: {input_to_use}")
    
    result = agent_executor.invoke({
        "input": input_to_use,
        "chat_history": state["chat_history"]
    })
    
    # ì¤‘ê°„ ë‹¨ê³„ì™€ ì¶œì²˜ ì •ë³´ í¬í•¨
    df = state.get("df")
    sheet_info = ""
    if df is not None:
        # ì‹¤ì œ ì‚¬ìš©ëœ ì»¬ëŸ¼ë“¤ì„ ì¶”ì • (í‚¤ì›Œë“œ ê¸°ë°˜)
        question = state["input"].lower()
        used_columns = []
        for col in df.columns:
            if any(keyword in col.lower() for keyword in ["ë§¤ì¶œ", "íŒë§¤", "ìˆ˜ëŸ‰", "ì˜ì—…ì´ìµ", "ì‚¬ì—…ì‹¤", "ê·¸ë£¹"]):
                used_columns.append(col)
        
        if used_columns:
            sheet_info = f" (ì£¼ìš” ì°¸ê³  ì»¬ëŸ¼: {', '.join(used_columns[:5])})"
    
    output_with_details = {
        "output": result["output"],
        "intermediate_steps": result.get("intermediate_steps", []),
        "source_info": f"ë°ì´í„° ì¶œì²˜: ì—…ë¡œë“œëœ ì—‘ì…€ íŒŒì¼{sheet_info}"
    }
    
    return output_with_details

def fallback_node(state: AgentState) -> dict:
    """ë¶„ì„ í‚¤ì›Œë“œê°€ ì—†ì„ ë•Œ ì‘ë‹µí•˜ëŠ” í´ë°± ë…¸ë“œì…ë‹ˆë‹¤."""
    print("--- í´ë°± ë…¸ë“œ ì‹¤í–‰ ---")
    return {"output": "ì£„ì†¡í•©ë‹ˆë‹¤. ì´í•´í•  ìˆ˜ ìˆëŠ” ë¶„ì„ í‚¤ì›Œë“œë¥¼ ì°¾ì§€ ëª»í–ˆì–´ìš”. 'ì‚¬ì—…ë¶€', 'ë§¤ì¶œ' ë“±ì˜ í‚¤ì›Œë“œë¥¼ ì‚¬ìš©í•´ ë‹¤ì‹œ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”."}

def create_graph_workflow(agent_executor: AgentExecutor) -> StateGraph:
    """í–¥ìƒëœ LangGraph ì›Œí¬í”Œë¡œìš°ë¥¼ ìƒì„±í•˜ê³  ì»´íŒŒì¼ëœ ì‹¤í–‰ê¸°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    workflow = StateGraph(AgentState)

    # ë…¸ë“œë“¤ ì¶”ê°€
    workflow.add_node("router_node", router_node)
    workflow.add_node("context_aware_node", context_aware_node)
    workflow.add_node("multi_context_aware_node", multi_context_aware_node)
    workflow.add_node("intent_classification_node", intent_classification_node)
    workflow.add_node("multi_intent_classification_node", multi_intent_classification_node)
    workflow.add_node("query_planning_node", query_planning_node)
    workflow.add_node("multi_query_planning_node", multi_query_planning_node)
    workflow.add_node("dataset_routing_node", dataset_routing_node)
    
    # ë°ì´í„°ì…‹ë³„ ì „ìš© ë…¸ë“œë“¤
    workflow.add_node("single_dataset_agent", lambda state: single_dataset_agent(state, agent_executor))
    workflow.add_node("multi_dataset_agent", lambda state: multi_dataset_agent(state, agent_executor))

    # ì›Œí¬í”Œë¡œìš° êµ¬ì„±: Router â†’ Dataset Routing â†’ ê° ê²½ë¡œë³„ ìµœì í™”ëœ ì²˜ë¦¬
    workflow.set_entry_point("router_node")
    workflow.add_edge("router_node", "dataset_routing_node")

    # Dataset Routing ê²°ê³¼ì— ë”°ë¥¸ ë¶„ê¸° (ì¡°ê¸° ë¶„ê¸°)
    workflow.add_conditional_edges(
        "dataset_routing_node",
        dataset_routing_node,
        {
            "single_dataset_path": "context_aware_node",  # ë‹¨ì¼ íŒŒì¼: ê¸°ì¡´ íŒŒì´í”„ë¼ì¸
            "multi_dataset_path": "multi_context_aware_node"  # ë‹¤ì¤‘ íŒŒì¼: ì „ìš© íŒŒì´í”„ë¼ì¸
        }
    )
    
    # ë‹¨ì¼ íŒŒì¼ ê²½ë¡œ: ê¸°ì¡´ íŒŒì´í”„ë¼ì¸
    workflow.add_edge("context_aware_node", "intent_classification_node")
    workflow.add_edge("intent_classification_node", "query_planning_node")
    workflow.add_edge("query_planning_node", "single_dataset_agent")
    
    # ë‹¤ì¤‘ íŒŒì¼ ê²½ë¡œ: ì™„ì „í•œ íŒŒì´í”„ë¼ì¸ (Intent/Query Planning í¬í•¨)
    workflow.add_edge("multi_context_aware_node", "multi_intent_classification_node")
    workflow.add_edge("multi_intent_classification_node", "multi_query_planning_node")  
    workflow.add_edge("multi_query_planning_node", "multi_dataset_agent")
    
    # ì¢…ë£Œ ì—£ì§€ë“¤
    workflow.add_edge("single_dataset_agent", END)
    workflow.add_edge("multi_dataset_agent", END)

    return workflow.compile()

# --- 4. ë©”ì¸ ì‹¤í–‰ (Main Execution) ---

def main():
    """ìŠ¤í¬ë¦½íŠ¸ì˜ ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ì…ë‹ˆë‹¤."""
    load_dotenv()
    openai_api_key = os.getenv("OPENAI_API_KEY")
    if not openai_api_key:
        raise ValueError("OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
    # íˆ´ ë¡œë“œ
    from agent.tool_registry import registered_tools

    # 1. ì—ì´ì „íŠ¸ ì‹¤í–‰ê¸° ìƒì„±
    agent_executor = create_agent_executor(openai_api_key, registered_tools)

    # 2. ê·¸ë˜í”„ ì›Œí¬í”Œë¡œìš° ìƒì„±
    app = create_graph_workflow(agent_executor)

# The code snippet you provided is a part of the main execution function in a Python script. Here's
# what it does:
    # # 3. í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    # print("\n--- [Test Case 1: ì—ì´ì „íŠ¸ ê²½ë¡œ] ---")
    # inputs_agent = {"input": "2023ë…„ ì‚¬ì—…ë¶€ë³„ ë§¤ì¶œ ì•Œë ¤ì¤˜", "chat_history": []}
    # result_agent = app.invoke(inputs_agent)
    # print("\n[ìµœì¢… ê²°ê³¼]")
    # print(result_agent['output'])

    # print("\n\n--- [Test Case 2: í´ë°± ê²½ë¡œ] ---")
    # inputs_fallback = {"input": "ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œ?", "chat_history": []}
    # result_fallback = app.invoke(inputs_fallback)
    # print("\n[ìµœì¢… ê²°ê³¼]")
    # print(result_fallback['output'])

# Create the graph executor for LangGraph Studio
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")
if openai_api_key:
    from agent.tool_registry import registered_tools
    agent_executor = create_agent_executor(openai_api_key, registered_tools)
    graph_executor = create_graph_workflow(agent_executor)
else:
    print("Warning: OPENAI_API_KEY not found. Graph executor not initialized.")
    graph_executor = None

if __name__ == "__main__":
    main()